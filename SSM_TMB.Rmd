---
title: "Fitting state-space models using the R package TMB"
author: "Marie Auger-Méthé"
date: "28/02/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Document info

This tutorial is heavily based on Appendix S1 (Supporting Information) from Auger-Méthé, M., Newman, K., Cole, D., Empacher, F., Gryba, R., King, A. A., Leos-Barajas, V., Mills Flemming, J., Nielsen, A., Petris, G., and Thomas, L.. 2021. A guide to state–space modeling of ecological time series. Ecological Monographs 91(4):e01470. 10.1002/ecm.1470 (Open access,  https://doi.org/10.1002/ecm.1470). Please refer to the paper and its appendices for more in-depth information on state-space models and how to apply them to data using R.

# Important installation info (to do before seminar!)

Because `TMB` use C++ code, the installation of the package requires additional steps. The package is available on CRAN https://cran.r-project.org/web/packages/TMB/index.html, however your computer needs to be set up so you can compile the C++ code. You can find installation information at: https://github.com/kaskr/adcomp/wiki/Download. Briefly, if you are Mac user, you will need to install the Xcode developer tools (freely available via the App store). With older versions of Mac OS, you may run into problems with the Fortran compiler. For solutions, see the link above and this link https://mac.r-project.org/tools/. If you are a Windows user, you will need to install Rtools: https://cran.r-project.org/bin/windows/Rtools/. You can find guidelines on how to install `TMB` on a Windows machine here: https://github.com/kaskr/adcomp/wiki/Windows-installation.


# Introducing the method with a simple univariate linear Gaussian state-space models

## Model

We start by exploring a very simple SSM: a simple linear SSM with Normal distributions. This model is not linked to an ecological example; it's just a teaching tool. This linear Gaussian SSM
consists of two equations for two time series.

The process equation is:
\begin{equation}
  z_t = z_{t-1} + \epsilon_t, \;\;\; \epsilon_t \sim \text{N}(0, \sigma_p^2),
  \label{E.toy2p.p} 
\end{equation}
where $z_t$ is the state value at time $t$, for $t=1, ..., T$. The states are generally unknown, i.e., cannot be observed directly, and are sometimes referred to as latent states. This equation represents the evolution of the hidden state as a correlated random walk. The equation implies that there is some stochasticity in the process. This stochasticity is often referred as process variation and is here described by a Normal distribution with standard deviation $\sigma_p$. For simplicity, we set the initial state value to be 0, i.e., $z_0 =0$.

The observation equation is:
\begin{equation}
  y_t = z_{t} + \eta_t, \;\;\; \eta_t \sim \text{N}(0, \sigma_o^2),
  \label{E.toy2p.o}
\end{equation}
where $y_t$ is the observation at time $t$, for $t=1, ..., T$. This equation links the observation at time $t$ to the underlying state at that time. The equation implies that we are observing the process with error. This error term is often referred as the observation error and, here, is described by a Normal distribution with standard deviation $\sigma_o$.

This model has two parameters: $\sigma_o, \sigma_p$.

## Simulations

Here, we simulate data using the linear Gaussian SSM described in Eqs. \ref{E.toy2p.p}-\ref{E.toy2p.o} from section \ref{LGSSMm} above. This is the simpler model where two of the parameters are fixed ($\alpha=\beta=1$).

First, let us simulate the process for a time series of length 200 ($T=200$), with an additional time step for the state at $t=0$. To be consistent with the model description, we set to $z_0 = 0$. We choose the standard deviation of the process variation, $\sigma_p$, to be 0.1.

```{r process.sim, tidy=FALSE}
# Create a vector that will keep track of the states
# It's of length T + 1 (+1 for t=0)
# T is not a good name in R, because of T/F, so we use TT
TT <- 200
z <- numeric(TT + 1)
# Standard deviation of the process variation
sdp <- 0.1
# Set the seed, so we can reproduce the results
set.seed(553)
# For-loop that simulates the state through time, using i instead of t,
for(i in 1:TT){
  # This is the process equation
  z[i+1] <- z[i] + rnorm(1, 0, sdp)
  # Note that this index is shifted compared to equation in text,
  # because we assume the first value to be at time 0
}
```

Let us plot the time series we have created.

```{r process.sim.fig}
plot(0:TT, z,
     pch = 19, cex = 0.7, col="red", ty = "o", 
     xlab = "t", ylab = expression(z[t]), las=1)
```

Second, let us simulate the observations. We set the standard deviation of the observation error $\sigma_o$ to 0.1.

```{r obs.sim, tidy=FALSE}
# Create a vector that will keep track of the observations
# It's of length T
y <- numeric(TT)
# Standard deviation of the observation error
sdo <- 0.1
# For t=1, ... T, add measurement error
# Remember that z[1] is t=0
y <- z[2:(TT+1)] + rnorm(TT, 0, sdo)
```

Let us plot both the observations and the states. From now on, we are adding extra space on the y-axis to leave space for the legend. Note that the space we assigned may not work for all figure sizes.

```{r obs.sim.fig, tidy=FALSE}
plot(1:TT, y,
     pch=3, cex = 0.8, col="blue", ty="o", lty = 3,
     xlab = "t", ylab = expression(z[t]),
     xlim = c(0,TT), ylim = c(min(y), max(y)+max(y)/5),
     las = 1)
points(0:TT, z,
       pch = 19, cex = 0.7, col="red", ty = "o")
legend("top",
       legend = c("Obs.", "True states"),
       pch = c(3, 19),
       col = c("blue", "red"),
       lty = c(3, 1),
       horiz=TRUE, bty="n", cex=0.9)
```

With real data, we usually only have the observations, $y_t$, and do not know the true states, $z_t$. However, simulated data allow us to see the discrepancies between the observed values and the values for the process they represent.

## Fitting the model using TMB

In TMB you need to create a C++ file that computes the value of the negative log likelihood for your data and for a given set of parameter values. Then, you compile the C++ code using `TMB` and use that function in one of R's optimizers to find the minimum negative log likelihood. The beauty of `TMB` is that it uses the Laplace approximation to integrate over the states and computes the gradient efficiently, which speeds up the optimizing process.


Assuming you have properly installed \texttt{TMB}, let's load it.

```{R LoadTMB, message=FALSE}
library(TMB)
```

### Writing the negative log-likelihood function in C++

Now, let us create the negative log-likelihood function that we will minimize (equivalent of maximizing the likelihood). We write this function in a special TMB C++ language. The code for the function needs to be save in a text file. We usually give it the extension .cpp. You can do this directly in R Studio. You write the code just as you would write a separate R script, but you save it as a C++ file by giving it the .cpp extension.

We name the file **toy2p.cpp** and it will contain the following code. Note that in C++, comments are preceded by // or contained within /* */. Also, as explained below, here the code is a little bit more complex than strictly necessary. We have included code that allow us to compute the one-step-ahead residuals and simulate from the model. While these features are not always necessary, they are key to model checking and thus we believe they should be part of all model fitting workflow.

```{Rcpp toy2p.cpp, eval=FALSE}
/*----------------------- SECTION A --------------------------*/
// Link to the TMB package
#include <TMB.hpp>

/*----------------------- SECTION B --------------------------*/
// Define main function
template<class Type>
Type objective_function<Type>::operator() ()
{

  /*----------------------- SECTION C --------------------------*/
  // Specify the input data
  DATA_VECTOR(y);

  // For one-step-ahead residuals
  DATA_VECTOR_INDICATOR(keep, y);

  // Specify the parameters
  PARAMETER(logSdP); // Log of st. dev. for the process variation
  PARAMETER(logSdO); // Log of st. dev. for the observation error

  // Specify the random effect/states
  PARAMETER_VECTOR(z);

  /*----------------------- SECTION D --------------------------*/
  // Transform standard deviations
  // exp(par) is a trick to make sure that the estimated sd > 0
  Type sdp = exp(logSdP);
  Type sdo = exp(logSdO);

  /*----------------------- SECTION E --------------------------*/
  // Define the variable that will keep track of
  // the negative log-likelihood (nll)
  Type nll = 0.0;

  /*----------------------- SECTION F --------------------------*/
  // Calculate the contribution to the negative log-likelihood
  // of the process equation for t=1,...,T
  // Remember that we fixed z_0 = 0
  for(int i = 1; i < z.size(); ++i){
    nll -= dnorm(z(i), z(i-1), sdp, true);

    //*----------------------- SECTION G --------------------------*/
    // Simulation block for process equation
    SIMULATE {
      z(i) = rnorm(z(i-1), sdp);
    }
  }

  /*----------------------- SECTION H --------------------------*/
  // Calculate the contribution to the negative log-likelihood
  // of the observation equation for t=1,...,T
  // Remember, the first element of z is at t=0,
  // while the first element of y is at t=1
  for(int i = 0; i < y.size(); ++i){
    nll -= keep(i)*dnorm(y(i), z(i+1), sdo, true);

    //*----------------------- SECTION I --------------------------*/
    // Simulation block for observation equation
    SIMULATE {
      y(i) = rnorm(z(i+1), sdo);
    }
  }


  /*----------------------- SECTION J --------------------------*/
  // State the transformed parameters to report
  // Using ADREPORT will return the point values and the standard errors
  // Note that we only need to specify this for parameters
  // we transformed, see section D above
  // The other parameters, including the random effects (states),
  // will be returned automatically
  ADREPORT(sdp);
  ADREPORT(sdo);

  /*----------------------- SECTION K --------------------------*/
  // Report simulated values
  SIMULATE{
    REPORT(z);
    REPORT(y);
  }


  /*----------------------- SECTION L --------------------------*/
  // State that we want the negative log-likelihood to be returned
  // This is what the optimizer in R will minimize
  return nll;

}

```


A few notes about C++:

- C++ uses round brackets `()` for both objects and functions

- the index of an object in C++ starts at 0, not 1. For example, to get the first element of a vector called `x`, you write `x(0)`. This is why the for-loops in sections F and H end before the length of the index, which is specified by `.size()`. If R crashes when you compile the code (see below for compiling instruction), the most likely culprit is an index problem, so check your for-loops.

- lines end with `;`

- Many of the density functions available in R are available in C++, at least when using the `TMB package`. These usually have the same names and arguments as those in R. For example, in sections F and H we use `dnorm` just like we would in R.

A few tips about writing your function:

- If we want to use an unconstrained optimizer in R, we have to specify in the negative log-likelihood function that we transform the values provided by the optimizer into values that are in the appropriate domain. For example, here in  section D, we exponentiate the input log-transformed standard deviations (logSdP and logSdO) to ensure that the standard deviations have positive values. We do this so that we can use an unconstrained optimizer, yet only have positive estimated values for the standard deviations.

- By default, `TMB` will only return the estimated values in the same form as given in the function (e.g., `logSdP` rather than `sdp`). To be able to retrieve the estimate values of interest, we need to specify that we want them returned. This is done in section J.

- Note that we have included simulation blocks (sections G, I, and K), even though these blocks are not necessary to fit the model to data. These blocks are extremely useful for model validation, see the model validation section below for details.

- Similarly, while it is not necessary to fit the model, we have added an indicator variable (`keep`) in both sections C and H. This indicator variable is necessary to calculate the one-step-ahead residuals that we will discuss in the model validation section.

Now that a few things are defined, it may be easier to go back to the function and see what it means. 

- Section A loads the packages needed. 

- Section B indicates that we are defining the main function that will calculate the negative log likelihood. 

- Section C defines the data and parameters. The values of both data and parameters will be put in through R. 

- Section D transforms the parameters so that they are constrained appropriately (here, that the standard deviation are $>0$). 

- Section E creates the object that will keep track of the negative log likelihood. 

- Section F has a for-loop that calculates the contribution to the negative log likelihood of the process equation. 

- Section G allows to simulate the process equation.

- Section H calculates the contribution to the negative log likelihood of the observation equation. Note that using \texttt{keep} tells \texttt{TMB} that this is where we would remove the data for the one-step ahead residuals. 

- Section I allows to simulate the observation equation. 

- Section J indicates that we want the parameters in their untransformed form to be returned with their standard errors. 

- Section K indicates what to report when we simulate the model. 

- Section L indicates that the main item the function returns is the negative log likelihood.

Now that we have the negative log likelihood C++ file saved, we go back to R.

### Compiling and loading C++ function

The first thing we want to do is compile the C++ code and load it so we can use it to calculate the negative log likelihood of our model. Here, our **toy2p.cpp** file is found in the working directory.

```{r tmbCompile,  results="hide"}
compile("toy2p.cpp", flags="-Wno-unused-variable")
dyn.load(dynlib("toy2p"))
```

### Preparing the model for fitting

First, we need to prepare the data. This is going to be a list with all the items found in the .cpp files that are of the type `DATA` (e.g., `DATA\_VECTOR`). Here, we only have \texttt{y} which is the time series of observed values.

```{r prepY}
dataTmb <- list(y = y)
```

Second, we need a list with the parameters. Again the names need to match those in the .cpp file. These are the starting values for the parameters. Note that this includes both the parameters and the states.

```{r parPrep, tidy=FALSE}
par2pTmb <- list(logSdP = 0, logSdO = 0,
             z=rep(0,length(dataTmb$y)+1))
```

By default all values of `par2pTmb` will be estimated, but we assume that we know the value of the state at time 0, $z_0 = 0$. To provide this information, we can create a named list that we will input for the argument `map` of the function `MakeADFun` below. The elements of `map` should have the same name and size as those in `par2pTmb`, but we only need to specify the elements that we want to manipulate, here `par2pTmb\$z`. To fix a value, we set that value to `NA`. The values that are estimated should have a different entry and all values should be factors.

```{r tmbMapPrep}
mapTmb <- list(z=as.factor(c(NA,1:length(dataTmb$y))))
```

Before we can fit the model, we need to use the function `MakeADFun` to combine the data, the parameters, and the model and create a function that calculates the negative
log likelihood and the gradients. 

To identify our random effects, namely the states, $z_t$, we set the argument `random` to equal the name of the parameters that are random effects. The argument `DLL` identify the compiled C++ function to be linked.

```{r MakeADFun, tidy=FALSE}
m2pTmb <- MakeADFun(data=dataTmb, 
                    parameters=par2pTmb, 
                    map=mapTmb, 
                    random= "z",
                    DLL= "toy2p")
```

### Fitting the model to data

We fit the model using \texttt{nlminb}, which is a base R optimizer, but we input the object returned by `MakeADFun`.

```{r tmbOpt}
f2pTmb <- nlminb(start = m2pTmb$par, # Initial values for the parameters
                 objective = m2pTmb$fn, # Function to be minimized
                 gradient = m2pTmb$gr) # Gradient of the objective
```

It is important to check whether the model converged.

```{r}
f2pTmb$message
```

A message stating \texttt{"both X-convergence and relative convergence (5)"} would also indicate convergence.

### Exploring the results

To look at the parameter estimates, you can use the function `sdreport` and object object created by `MakeADFun`.

```{r tmbEst, tidy=FALSE}
sdr2pTmb <- summary(sdreport(m2pTmb))
# This will have the parameters and states
# So we can just print the parameters
sdr2pTmb[c("sdp", "sdo"),]
```

We can see that the estimates for both the process variation and the observation error are close to their true value of 0.1, with relatively small standard errors (SEs).

We can get the smoothed state values with the `sdreport` function as above and this will give you the SEs for these states. If you are only interested in the state values (not theis SEs), you can also extract them directly from the model object.

```{r}
# To get the point estimate and the SE of the states
zsSe2pTmb <- sdr2pTmb[row.names(sdr2pTmb)=="z",]
head(zsSe2pTmb)
# To get only the point estimates of the states
zs2pTmb <- m2pTmb$env$parList()$z
head(zs2pTmb)
```

Note that in this case, because we fixed the value of the state at time $t=0$, the first method, which looks at the predicted values, only returns state values for $t>0$.

We can use the SEs to calculate the 95\% confidence intervals.

```{r tmbCI, tidy=FALSE}
zsCIl2pTmb <- zsSe2pTmb[,1] + 
  qnorm(0.025, sd = zsSe2pTmb[,2])
zsCIu2pTmb <- zsSe2pTmb[,1] + 
  qnorm(0.975, sd = zsSe2pTmb[,2])
```

We can now overlay the state estimates along with their confidence intervals on the plot of the simulated data.

```{R tmbRp, tidy=FALSE}
plot(1:TT, y,
     pch=3, cex=0.8, col="blue", ty="o", lty = 3,
     xlab = "t", ylab = expression(z[t]),
     xlim = c(0,TT), ylim = c(min(y), max(y)+max(y)/5),
     las = 1)
points(0:TT, z,
       pch=19, cex=0.7, col = "red", ty="o")
polygon(c(1:TT, TT:1), c(zsCIl2pTmb,rev(zsCIu2pTmb)),
        col=rgb(1,0.7,0.4,0.3), border=FALSE)
lines(0:TT, zs2pTmb, 
      col= "darkgoldenrod1", lwd = 2)
legend("top",
       legend = c("Obs.", "True states", "Smooth. states"),
       pch = c(3, 19, NA),
       col = c("blue", "red", "darkgoldenrod1"),
       lwd = c(1, 1, 2), lty = c(3, 1, 1),
       horiz=TRUE, bty="n", cex=0.9)
```

### Model selection

We sometimes have more than one model to explore. For example, we could fit the more general and slightly more complex version of the model, where we are estimating in addition to the standard
deviations, two additional parameter: $\alpha$ and $\beta$. 

The process equation of this new model is:
\begin{equation}
  z_t = \beta z_{t-1} + \epsilon_t, \;\;\; \epsilon_t \sim \text{N}(0, \sigma_p^2),
  \label{E.toy4p.p}
\end{equation}
where $\beta$ represents the autocorrelation in the state values. 

The observation equation of this new model is:
\begin{equation}
  y_t = \alpha z_{t} + \eta_t, \;\;\; \eta_t \sim \text{N}(0, \sigma_o^2),
  \label{E.toy4p.o}
\end{equation}
where $\alpha$ is a constant of proportionality that represents any systematic discrepancy between the observations and the states (e.g., the average detection rate).

In our simulation, both $\alpha$ and $\beta$ are implicitly set to 1. Here we estimate these values.

To fit this model in ```TMB```, we will create a new file called **toy4p.cpp**. Note that this is exactly the same as **toy2p.cpp** except that we added the two new parameters in sections C, and F to I.

```{Rcpp toy4p.cpp, write_chunk=TRUE, eval=FALSE}
/*----------------------- SECTION A --------------------------*/
// State that we need the TMB package
#include <TMB.hpp>

/*----------------------- SECTION B --------------------------*/
// Define main function
template<class Type>
Type objective_function<Type>::operator() ()
{

  /*----------------------- SECTION C --------------------------*/
  // Specify the input data
  DATA_VECTOR(y);

  // For one-step-ahead residuals
  DATA_VECTOR_INDICATOR(keep, y);

  // Specify the parameters
  PARAMETER(logSdP); // Log of st. dev. for the process variation
  PARAMETER(logSdO); // Log of st. dev. for the observation error
  PARAMETER(alpha);
  PARAMETER(beta);

  // Specify the random effect/states
  PARAMETER_VECTOR(z);

  /*----------------------- SECTION D --------------------------*/
  // Transform standard deviations
  // exp(par) is a trick to make sure that the estimated sd > 0
  Type sdp = exp(logSdP);
  Type sdo = exp(logSdO);

  /*----------------------- SECTION E --------------------------*/
  // Define the variable that will keep track of
  // the negative log-likelihood (nll)
  Type nll = 0.0;

  /*----------------------- SECTION F --------------------------*/
  // Calculate the contribution to the negative log-likelihood
  // of the process equation for t=1,...,T
  // Remember that we fixed z_0 = 0
  for(int i = 1; i < z.size(); ++i){
    nll -= dnorm(z(i), beta*z(i-1), sdp, true);

    //*----------------------- SECTION G --------------------------*/
    // Simulation block for process equation
    SIMULATE {
      z(i) = rnorm(beta*z(i-1), sdp);
    }
  }

  /*----------------------- SECTION H --------------------------*/
  // Calculate the contribution to the negative log-likelihood
  // of the observation equation for t=1,...,T
  // Remember, the first element of z is at t=0,
  // while the first element of y is at t=1
  for(int i = 0; i < y.size(); ++i){
    nll -= keep(i)*dnorm(y(i), alpha*z(i+1), sdo, true);

    //*----------------------- SECTION I --------------------------*/
    // Simulation block for observation equation
    SIMULATE {
      y(i) = rnorm(alpha*z(i+1), sdo);
    }
  }

  /*----------------------- SECTION J --------------------------*/
  // State the transformed parameters to report
  // Using ADREPORT will return the point values and the standard errors
  // Note that we only need to specify this for parameters
  // we transformed, see section D above
  // The other parameters, including the random effects (states),
  // will be returned automatically
  ADREPORT(sdp);
  ADREPORT(sdo);

  /*----------------------- SECTION K --------------------------*/
  // Report simulated values
  SIMULATE{
    REPORT(z);
    REPORT(y);
  }


  /*----------------------- SECTION L --------------------------*/
  // State that we want the negative log-likelihood to be returned
  // This is what the optimizer in R will minimize
  return nll;

}

```

As before, we compile and load the function.

```{r tmbfCompile, results="hide"}
compile("toy4p.cpp", flags="-Wno-unused-variable")
dyn.load(dynlib("toy4p"))
```

We can use the same data and map objects as above, but we need to change the object containing the initial parameter values, because we have two new parameters to estimate. 

We then get the MLE. Note that while we can often use uninformative starting values (e.g., 0), we can run into estimation problems if we set all the starting values as 0s. To avoid these problems, we use the true simulated values for the starting values. With real data, we could have started with sensible values rather than 0.

```{r tmbfMLE, tidy=FALSE}
par4pTmb <- list(logSdP = log(0.1), logSdO = log(0.1), 
                 alpha = 1, beta = 1, # New parameters
                 z=rep(0,length(dataTmb$y)+1))
m4pTmb <- MakeADFun(dataTmb, par4pTmb, map=mapTmb, random= "z",
                  DLL= "toy4p", silent=TRUE)
f4pTmb <- nlminb(m4pTmb$par, m4pTmb$fn, m4pTmb$gr)
f4pTmb$message
```

We can see here that the optimizer has converged.

We can use again `sdreport` to get the parameter estimates and their SEs.

```{r tmbfest, tidy=FALSE}
sdr4pTmb <- summary(sdreport(m4pTmb))
cbind(Simulated = c(sdp, sdo, 1, 1),
      sdr4pTmb[c("sdp", "sdo", "beta", "alpha"),])
```

We can see here that compared to the estimates from the original model, the more complex and flexible model has much
higher standard errors. As mentioned above, the optimization process is also quite sensitive to the starting values.

We can use AIC to compare these two models. Since the \texttt{C++} function calculates the negative log likelihood and is the objective of the optimization procedures, we can just use the returned value by `nlminb`.

```{r tmbAIC, tidy=FALSE}
aic2pTmb <- 2 * 2 + 2 * f2pTmb$objective
aic4pTmb <- 2 * 4 + 2 * f4pTmb$objective
c(Original = aic2pTmb, 
  Flexible = aic4pTmb,
  Difference = aic4pTmb - aic2pTmb)
```

We can see that the original model has a lower AIC than the more flexible model. Thus the original model is considered better. If the two models were equivalent, we would expect a AIC difference of 4 ($2k$), but here the difference is much smaller: \Sexpr{round(aic4pTmb - aic2pTmb,2)}. This may be because the more flexible model is overfitting the data.

### Checking estimability and model fit with simulations

As mentioned above, it is good practice to include simulation blocks
within the C++ code because it can be advantageous to use `TMB` simulation functions to simulate directly from the model we fit to data. This can speed
up the process and, as we will see below, can help assess the model fit. 

`TMB` has various simulators that have the same naming convention as those in R  (e.g., `rnorm()`, `rpois()`).

In the \emph{toy2p.cpp} file we created above, we have three simulation blocks:

- Section G simulates the process equation,

- Section I simulates the observation equations, and

- Section K identifies the simulated values to return.

If we want to recreate the exact same simulation as we did above, our first step is to create a model object with `MakeADFun`. 

We need to set up the parameter and data object. We will create a data object with 0s, which has the length of the time series we want to create. 

```{r sim.tmb.data, tidy=FALSE}
dataSimTmb <- list(y = numeric(TT))
```

Instead of using 0 as parameter values for the two standard deviations, we will set these values to 0.1, the values used in the original simulations above.
For the states, we will create a vector of 0 of the length of our state vector. 
```{r sim.tmb.par, tidy=FALSE}
parSimTmb <- list(logSdP = log(0.1), logSdO = log(0.1),
             z=rep(0,length(dataSimTmb$y)+1))
```

We will then create the \texttt{TMB} object using \texttt{MakeADFun} function, just as above. We will use the same \texttt{map} object as above to tell \texttt{TMB} that the initial state value is $z_0 = 0$.

```{r tmbSimLGSSMMAF, tidy=FALSE}
mSimTmb <- MakeADFun(dataSimTmb, parSimTmb, map=mapTmb, random= "z",
                  DLL= "toy2p", silent=TRUE)
```

Finally, we will simulate the data using the using the created object and its `\$simulate()`.

```{r tmbSimLGSSM}
set.seed(553) # Same random seed as above
simTmb <- mSimTmb$simulate()
```

Both the observations `y` and the states `z` are returned. This is because
section K of the C++ code specifically states to return both. Because we used the same random seed as for the original simulation, you will get the same results.

How can these simulation blocks be useful for model verification?

Simulations can be used to assess whether we can estimate the parameter and states. Here, once you have define you \texttt{C++} code appropriately, you can simulate and check that your model can estimate the parameter and states in only a few lines of code (note some code is repeated from above).

```{R tmbSimCheck, tidy=FALSE}
# Simulate the data (same code as above)
set.seed(553)
dataSimTmb <- list(y = numeric(TT))
parSimTmb <- list(logSdP = log(0.1), logSdO = log(0.1),
             z=rep(0,length(dataSimTmb$y)+1))
mSimTmb <- MakeADFun(dataSimTmb,
                     parSimTmb, map=mapTmb, random= "z",
                  DLL= "toy2p", silent=TRUE)
simTmb <- mSimTmb$simulate()
# Fit the model to check estimation
mCheckTmb <- MakeADFun(list(y = simTmb$y), # Note difference in this argument
                       parSimTmb, map=mapTmb, random= "z",
                  DLL= "toy2p", silent=TRUE)
fCheckTmb <- nlminb(mCheckTmb$par, mCheckTmb$fn, mCheckTmb$gr)
fCheckTmb$message
# Compare simulated and estimated parameters
cbind("True" = parSimTmb[1:2], "Est" = fCheckTmb$par)
# Compare simulated and estimated states - difference
summary(abs(simTmb$z - mCheckTmb$env$parList()$z))
```

As we can see, the parameter (here untransformed) and the state estimates are close to the simulated value. This is a quick check here, but one could look at easily take a deeper look using Root Mean Square prediction errors (see Auger-Méthé et al. 2021).

Another simple, albeit often conservative, way to look at the model fit using simulations is to use the frequentist equivalent of posterior predictive measures. The idea is to define a test statistic (e.g., the mean of $\mathbf{y}$) and calculate how probable more extreme values of this test statistic would be if we were to sample from our model compared to the observed test statistic value. Here, we will look at two test statistics: the mean $y_t$ and the standard deviation of $y_t$. Specifically, we will simulate a time series of observations 200 times and, for each replicate, calculate the mean and standard deviation of that replicate. We will then be able to compare the simulated test statistics to the observed test statistics.

Note here we will use the parameter values we estimated for this simple model above.

```{r tmbTestStat, tidy=FALSE}
nrep <- 200 # Number of replicates
set.seed(999) # Set the random seed to be able to reproduce the simulation
# Create matrix to keep track of replicate test statistic values
repsT <- matrix(NA, nrow=nrep, ncol=2)
colnames(repsT) <- c("Mean", "SD")
# Parameter to use to simulate
parSp <- m2pTmb$env$par
# Set to estimated values
parSp[1:2] <- f2pTmb$par
for(i in 1:nrep){ # For each replicate
  yrep <- mSimTmb$simulate(par=parSp)$y # simulate observations
  repsT[i, "Mean"] <- mean(yrep)
  repsT[i, "SD"] <- sd(yrep)
}
```

Then, we calculate the fraction of test statistic values calculated from the simulated data sets that are greater or equal to the test statistic value calculated from the observed data:
\begin{equation}
p_C = Pr(T(\mathbf{y}^{i}) \geq T(\mathbf{y})|\hat{\boldsymbol{\theta}}_\textsc{mle}),
\end{equation}
where $\hat{\boldsymbol{\theta}}_\textsc{mle}$ is the MLE value of $\boldsymbol{\theta}$, $T(\mathbf{y}^{i})$ is the test statistic value of a replicate and $T(\mathbf{y})$ is the test statistic value for the observed data. 

Extreme values close to 0 or 1 could be indicative of discrepancies between the model and the data. As here we are looking at the true model with the parameters estimated values that were very close to the true values
used to simulate the data, we would expect that the observed test statistics values are similar to that from the replicates.

```{R tmbpval}
# Calculate estimated pc
sum(repsT[,"Mean"] >= mean(y))/nrep
sum(repsT[,"SD"] >= sd(y))/nrep
```

Here, the values are by no means extreme and are quite central. As such, there is no evidence to suggest that the model is inadequate for the data. It is important to note that we are *not* accounting here for the fact that we are using the data twice: once to estimate the parameter values and once to assess the fit. Thus, such tests tend to be conservative and often fail to detect lack of fit. Thus, one may not want to rely too heavily on values such as these. But in some cases, they are useful to find large discrepancies between the model and the data. In general, to look at histograms of the simulated test statistic values, and see where the observed test statistic fall.

```{r tmbHist, tidy=FALSE}
layout(matrix(1:2,nrow=1))
hist(repsT[,"Mean"], breaks=30,
     main = "", las=1, col="darkgrey",
     xlab = expression(T(y^{i})))
box()
abline(v=mean(y), col="hotpink", lwd=4)
title("Mean Obs.", line=-1.5, adj=0.9)
hist(repsT[,"SD"], breaks=30,
     main="", las=1, col="darkgrey",
     xlab = expression(T(y^{i})))
box()
abline(v=sd(y), col="hotpink", lwd=4)
title("Sd Obs.", line=-1.5, adj=0.9)
```

We can see here that the observed test statistics are close to the main peaks from the histograms, which suggests that the observations are in line with the prediction of the model. But again, one needs to be careful, as these tests tend to be conservative.

### Effects of starting values and diagnostics for a mispecified model

Now let us explore whether the method described above could detect the lack of fit of model with poor parameter estimates. We will use this opportunity to also look at the effect of the starting values on the parameter and state estimates. For this example, we will use again the full more flexible model. However, in this case instead of using the true parameter values as starting values for the optimizer, we will use the default starting values of 0.

```{r tmbpvalbad, tidy = FALSE}
# Set the starting values
# Here we use 0s everywhere instead of the true values
parMisTmb <- list(logSdP = 0, logSdO = 0, alpha = 0, beta = 0,
              z=rep(0,length(dataSimTmb$y)+1))
# Make the TMB model function
# We use the original data and map (dataTmb, mapTmb)
mMisTmb <- MakeADFun(dataTmb, parMisTmb, map=mapTmb, random= "z",
                   DLL= "toy4p", silent=TRUE)
# Find the MLE - more like a local max
fMisTmb <- nlminb(mMisTmb$par, mMisTmb$fn, mMisTmb$gr)
# Converged
fMisTmb$message
```

It looks like the optimizer converged to a solution, but as we can see below, the parameter estimates based on the previous set of initial values and those used above are quite different. The fact that the parameter values for
$\alpha$ and $\beta$ are exactly the same as the input starting value should raise warning flags.

```{r tmbP0comp, tidy=FALSE}
c(New = fMisTmb$objective, Old = f4pTmb$objective)
rbind(New = fMisTmb$par, Old = f4pTmb$par)
```

To see how this estimation problem could affect the model fit, let us use again our frequentist version of the posterior predictive measures. 

```{r tmbpvalbad2, tidy = FALSE}
set.seed(999) # Set the random seed to be able to reproduce the simulation
# Create matrix to keep track of replicate test statistic values
repsB <- matrix(NA, nrow=nrep, ncol=2)
colnames(repsB) <- c("Mean", "SD")
# Parameter to use to simulate
parMisp <- mMisTmb$env$par
# Set to estimated values
parMisp[1:4] <- fMisTmb$par
for(i in 1:nrep){ # For each replicate
  yrep <- mMisTmb$simulate(par=parMisp)$y # simulate observations
  repsB[i, "Mean"] <- mean(yrep)
  repsB[i, "SD"] <- sd(yrep)
}
```

Now let us compare the simulated and observed test statistics visually.

```{r tmbHistpar0, tidy=FALSE}
layout(matrix(1:2,nrow=1))
# Simulated test statistics vs observed test statistics
hist(repsB[,"Mean"], breaks=30,
     main = "", las=1, col="darkgrey",
     xlab = expression(T(y^{i})),
     xlim = range(c(repsB[,"Mean"], c(0.9,1.1)*mean(y))))
box()
abline(v=mean(y), col="hotpink", lwd=4)
title("Mean Obs.", line=-1.5, adj=0.9)
hist(repsB[,"SD"], breaks=30,
     main = "", las=1, col="darkgrey",
     xlab = expression(T(y^{i})),
     xlim = range(c(repsB[,"SD"],  c(0.9,1.1)*sd(y))))
box()
abline(v=sd(y), col="hotpink", lwd=4)
title("Sd Obs.", line=-1.5, adj=0.9)
```

Clearly, the observed measures are quite different from those generated from the model with these parameter estimates. Thus, while we want to be careful when we fail to find a difference between the observed and simulated test statistics, finding such discrepancies is a clear sign of problem.
